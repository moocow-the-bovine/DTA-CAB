=encoding utf8

=pod

=head1 NAME

DTA::CAB:WebServiceTutorial - User tutorial for DTA::CAB web-service

=cut

##========================================================================
## DESCRIPTION
=pod

=head1 DESCRIPTION

This document describes the use of the L<DTA::CAB|DTA::CAB> (demo) web-service accessible
at L<http://www.deutschestextarchiv.de/demo/cab/>.
The L<DTA::CAB|DTA::CAB> web-service provides
error-tolerant linguistic analysis for historical German text,
including normalization of historical orthographic variants
to "canonical" modern forms using the method described in L<Jurish (2012)|http://opus.kobv.de/ubp/volltexte/2012/5578/>.
Due to legal restrictions on some of the underlying resources,
not all available analysis layers can be returned
by the publically accessible "demo" web-service, but it is hoped that the available
layers (linguistically salient TEI-XML serialization,
sentence- and word-level tokenization,
orthographic normalization,
part-of-speech tags, and (normalized) lemmata) should suffice for most purposes.

=cut

##========================================================================
## Interface Elements
=pod

=head2 Interface Elements

Upon accessing the top-level web-service URL ( L<http://www.deutschestextarchiv.de/demo/cab/> )
in a web browser, the user is presented with a graphical interface in which CAB queries can
be constructed and submitted to the underlying server.  This section describes the various
elements of that interface.

=over 4

=item Query Form

At the top of the web-service interface is a query form on a gray background including input fields for the CAB
query parameters ("Query", "Analyzer", "Format", etc.).

=item Status Line

Immediately beneath the query form is a status display line ("URL line") on a white background with no border,
which contains a link to the raw response data for the current query, if any.  In the case of
singleton (1-word) queries, the status line also contains
a simple heuristic "traffic-light" indicator of the query word's morphological security status, where
green indicates a "safe" known modern form, red indicates an unknown (assumedly historical) form,
and yellow indicates a known modern form which is judged unsafe for identity canonicalization (typically
a proper name).

=item Response Data

Immediately below the URL line is the response data area ("data area") on a white background with a gray border, which
displays the results for the current query, if any.

=item Link Buttons

Below the data area are a number of static link buttons for
the file upload demo ("File Demo") or the live user-input demo ("Live Demo"),
the list of analyzers supported by the underlying CAB instance ("Analyzers"),
the list of I/O formats supported by the underlying CAB instance ("Formats"),
administrative data for the CAB server instance ("Status"),
and the CAB documentation ("Documentation").

=item Footer

Below the link buttons is a short footer on a gray background containing
administrative information about the underlying CAB server.

=back

=cut

##========================================================================
## Basic Usage
=pod

=head2 Basic Usage

This section briefly describes the basic usage offered by the DTA::CAB
web-service by reference to some simple examples.

=cut

##========================================================================
## Basic Usage: Simple Query
=pod

=head3 A Simple Query

Most CAB parameters in the query form are initialized with sensible default values, with the exception
of the "Query" parameter itself, which should contain the text string to be analyzed.
Say we wish to analyze the text string "C<Elephanten>":
simply entering this string (or copy & paste it) into the text input box associated with the "Query" parameter,
and then pressing the <i>Enter</i> key or clicking the "submit" button will cause the query to be
submitted to the underlying CAB server and the response data to be displayed in the data area:
L<http://www.deutschestextarchiv.de/demo/cab/?q=Elephanten>

The results are displayed by default in CAB's native "L<Text|/Text>" format,
in which the first line contains the input surface form (C<Elephanten>), and the remaining lines are the CAB
attributes for the query word,
where each attribute line is indicated by an initial TAB character, a plus ("+") sign,
and the attribute label enclosed in square brackets ("[...]"), followed by the attribute value.
Useful attributes include "moot/word" (canonical modern form), "moot/tag" (part-of-speech tag), and
"moot/lemma" (modern lemma).

The example query for instance should produce a response such as:

 Elephanten
 	+[moot/word] Elefanten
 	+[moot/tag] NN
 	+[moot/lemma] Elefant

... indicating that the query was correctly normalized to the canonical modern form "Elefanten",
tagged as a common noun ("NN"), and assigned the correct canonical lemma "Elefant".

=cut

##========================================================================
## Basic Usage: Sentence Query
=pod

=head3 A Sentence Query

Suppose we wish to take advantage of context information while normalizing
a whole sentence of historical text, as described in 
L<Jurish (2012), Chapter 4|http://opus.kobv.de/ubp/volltexte/2012/5578/>.
Simply enter the entire text of the sentence to be analyzed in the "Query"
input, and ensure that the checkbox for the "tokenize" flag is checked,
and submit the query, e.g.

 EJn zamer Elephant gillt ohngefähr zweyhundert Thaler.

The output now contains multiple tokens (words), where the analysis for
each token begins with a line containing only its surface form (no leading whitespace).
Token attribute lines (introduced by a leading TAB character) refer to the token
most recently introduced.
The output for the example query can be directly accessed
L<here|http://www.deutschestextarchiv.de/demo/cab/?q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.>,
and should look something like the following:

 EJn
 	+[moot/word] Ein
 	+[moot/tag] ART
 	+[moot/lemma] eine
 zamer
 	+[moot/word] zahmer
 	+[moot/tag] ADJA
 	+[moot/lemma] zahm
 Elephant
 	+[moot/word] Elefant
 	+[moot/tag] NN
 	+[moot/lemma] Elefant
 gillt
 	+[moot/word] gilt
 	+[moot/tag] VVFIN
 	+[moot/lemma] gelten
 ohngefähr
 	+[moot/word] ungefähr
 	+[moot/tag] ADJD
 	+[moot/lemma] ungefähr
 zweyhundert
 	+[moot/word] zweihundert
 	+[moot/tag] CARD
 	+[moot/lemma] zweihundert
 Thaler
 	+[moot/word] Taler
 	+[moot/tag] NN
 	+[moot/lemma] Taler
 .
 	+[moot/word] .
 	+[moot/tag] $.
 	+[moot/lemma] .

=cut

##========================================================================
## Basic Usage: Multi-Sentence Query
=pod

=head3 A Multi-Sentence Query

The CAB web service can analyze multiple sentences as well, for example:

 EJn zamer Elephant gillt ohngefähr zweyhundert Thaler.
 Ceterum censeo Carthaginem esse delendam.

The corresponding output can be viewed L<here|http://www.deutschestextarchiv.de/demo/cab/?q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>, which should look something like:

 %% $s:lang=de
 EJn
 	+[moot/word] Ein
 	+[moot/tag] ART
 	+[moot/lemma] eine
 ...
 .
 	+[moot/word] .
 	+[moot/tag] $.
 	+[moot/lemma] .
 
 %% $s:lang=la
 Ceterum
 	+[moot/word] Ceterum
 	+[moot/tag] FM.la
 	+[moot/lemma] ceterum
 censeo
 	+[moot/word] censeo
 	+[moot/tag] FM.la
 	+[moot/lemma] censeo
 Carthaginem
 	+[moot/word] Carthaginem
 	+[moot/tag] FM.la
 	+[moot/lemma] carthaginem
 esse
 	+[moot/word] esse
 	+[moot/tag] FM.la
 	+[moot/lemma] esse
 delendam
 	+[moot/word] delendam
 	+[moot/tag] FM.la
 	+[moot/lemma] delendam
 .
 	+[moot/word] .
 	+[moot/tag] $.
 	+[moot/lemma] .


Here, blank lines indicate sentence boundaries, and comments (non-tokens) are lines
introduced by two percent signs ("%%").  The special comments immediately preceding each sentence
of the form "C<%% $s:lang=>I<LANG>" indicate the result of CAB's language-guessing module
L<DTA::CAB::Analyzer::LangId::Simple|DTA::CAB::Analyzer::LangId::Simple>.
The blank line between the final "." of the first sentence and the first word of the second
sentence indicates that the sentence boundary was correctly detected,
and the "C<%% $s.lang=>I<LANG>" comments indicate that the source language of both
sentences was correctly guessed ("de" indicating German in the former case, and "la" indicating
Latin in the latter).  Due to the language-guesser's assignment for the second sentence,
all words in that sentence are tagged as foreign material ("FM"), with the suffix ".la" indicating
the language-guesser's output.  Otherwise, no analysis (normalization or lemmatization) is performed
for sentences recognized as non-German.

=cut

##========================================================================
## Basic Usage: Document Query
=pod

=head3 A File Query

In addition to the default "live" query interface, the CAB web-service interface
also offers users the opportunity to upload an entire document file to be analyzed
and allowing the analysis results to be saved to the user's local machine.
The CAB file interface is accessible via the "File Demo"
button in the link area, which resolves to L<http://www.deutschestextarchiv.de/demo/cab/file|http://www.deutschestextarchiv.de/demo/cab/file>
Suppose we have a simple plain-text file L<F<elephant.raw>|http://./elephant.raw> containing the document to be analyzed:

 EJn zamer Elephant gillt ohngefähr zweyhundert Thaler.
 Ceterum censeo Carthaginem esse delendam.

First, save the input file L<F<elephant.raw>|http://./elephant.raw> to your local computer.
Then, in the CAB L<file input form|http://www.deutschestextarchiv.de/demo/cab/file>,
click on on the "Choose File" button and select F<elephant.raw> from wherever you saved it.
Clicking on the "submit" button will cause the contents of the selected file to be sent
to the CAB server, analyzed, and prompt you for a location to which the analysis results
should be saved (by default F<elephant.raw.txt>).
Assuming the default options were active, you should have a result file
resembling
L<this|http://www.deutschestextarchiv.de/demo/cab/query?q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>,
identical to the results displayed in the data area for
L<the multi-sentence query example|/A Multi-Sentence Query>.

Due to bandwidth limitations, the CAB server currently only accepts input files of size B<E<lt>= 512 KB>.
If you need analyze a large amount of data, you will first need to split your input files into
chunks of no more than 512 KB each, sending each chunk to the server individually.  In this case,
please refrain from "hammering" the CAB server with an uniterrupted stream of requests: wait at least
3-5 seconds between requests to avoid blocking the server for other users.  Alternatively,
if you need to analyze a large corpus, you can L<contact the I<Deutsches Textarchiv>|http://deutschestextarchiv.de/doku/impressum#kontakt>.

=cut


##========================================================================
## Analysis Chains
=pod

=head2 Analysis Chains

The CAB server supports a number of different analysis modes, corresponding to
different sorts of input data and/or different user tasks.  The various analysis
modes are implemented in terms of different analysis chains (a.k.a. "analyzers" or just "chains")
supported by the underlying analysis dispatcher class, L<DTA::CAB::Chain::DTA|DTA::CAB::Chain::DTA>.
The analysis mode to be used for a particular CAB request is specified by the
"analyzer" or "a" paramter, which is initially set to use the "default" analysis chain
(which is itself just an alias for the "norm" chain).

This section briefly describes some alternative analysis chains and situations in which
they might be useful.
For a full list of available analysis chains, see the list returned by the
"L<Analyzers|http://www.deutschestextarchiv.de/demo/cab/analyzers>" button in the link area,
and see L<DTA::CAB::Chain::DTA|DTA::CAB::Chain::DTA>
for a list of the available atomic analyzers and aliases for complex analysis chains.
For details on individual atomic analyzers, see the appropriate L<DTA::CAB::Analyzer|DTA::CAB::Analyzer>
subclass documentation.

=cut

##========================================================================
## Analysis Chains: Type-wise
=pod

=head3 Type-wise Analysis

As noted L<above|/"A Sentence Query">, the default "norm" analysis chain uses
sentential context to improve the precision of the normalization process
as described in L<Jurish (2012), Chapter 4|http://opus.kobv.de/ubp/volltexte/2012/5578/>.
This behavior is not always desirable, however.  In particular, if your data
is not arranged into linguistically meaningful sentence-like units -- e.g.
a simple flat list of surface types -- then no real context information is available,
and the "sentential" context CAB would use would more likely hinder the normalization
than help it.  For such cases, the "norm1" analysis chain can be employed instead
of the default "norm" chain.  The "norm1" chain uses only unigram-based probabilities
during normalization, so is less likely to be "confused" by non-sentence-like inputs.

Consider for example the input list:

 Fliegen fliegen nach

passing this list to the "norm1" chain inhibits context-dependent processing
and results in L<the following|http://www.deutschestextarchiv.de/demo/cab/?a=norm1&q=Fliegen+fliegen+nach>

 Fliegen
 	+[moot/word] Fliegen
 	+[moot/tag] NN
 	+[moot/lemma] Fliege
 fliegen
 	+[moot/word] fliegen
 	+[moot/tag] VVINF
 	+[moot/lemma] fliegen
 nach
 	+[moot/word] nach
 	+[moot/tag] APPR
 	+[moot/lemma] nach

contrast this to L<the output of the default "norm" chain|http://www.deutschestextarchiv.de/demo/cab/?a=norm&q=Fliegen+fliegen+nach.>:

 Fliegen
 	+[moot/word] Fliegen
 	+[moot/tag] NN
 	+[moot/lemma] Fliege
 fliegen
 	+[moot/word] fliegen
 	+[moot/tag] VVFIN
 	+[moot/lemma] fliegen
 nach
 	+[moot/word] nach
 	+[moot/tag] PTKVZ
 	+[moot/lemma] nach

In the above example, the second item "fliegen" is analyzed as an infinite verb (VVINF) for the unigram-based analyzer "norm1",
but as a finite verb (VVFIN) for the default analyzer.  Similarly, the final item "nach" is analyzed as a preposition (APPR) by
the unigram-based analyzer but as a verb particle (PTKVZ) by the default analyzer.  Although use of the "norm1" analyzer does
not alter either the canonical modern form or canonical lemma in this case, such cases are theoretically possible.

=cut

##========================================================================
## Analysis Chains: Expansion
=pod

=head3 Term Expansion

It is sometimes useful to have a list of all known orthographic variants of a given input form, e.g.
for runtime queries of a database which indexes only surface forms.  For such tasks, the analysis
chain "expand" can be used.  To
see all the variants of the surface form "Elephant" in the I<Deutsches Textarchiv> corpus for example, one could query
L<http://deutschestextarchiv.de/demo/cab/?a=expand&q=Elephant>, and expect a response something like:

 Elephant
 	+[moot/word] Elefant
 	+[moot/tag] NN
 	+[moot/lemma] Elefant
 	+[eqpho] Elephant <0>
 	+[eqpho] Elefant <14>
 	+[eqpho] elephant <17>
 	+[eqpho] elevant <17>
 	+[eqpho] Elephand <18>
 	+[eqpho] Elevant <18>
 	+[eqpho] elefant <18>
 	+[eqpho] Elephandt <19>
 	+[eqpho] Elephanth <19>
 	+[eqrw] Elefant <0>
 	+[eqrw] Elephant <0>
 	+[eqrw] Elephandt <8.44527626037598>
 	+[eqrw] elefant <8.44683265686035>
 	+[eqrw] Elephanth <8.70806312561035>
 	+[eqrw] elephant <9.01417255401611>
 	+[eqrw] Elephand <18.6624526977539>
 	+[eqrw] Eliphant <18.7045001983643>
 	+[eqrw] Elephants <21.1982593536377>
 	+[eqrw] elevant <21.3945064544678>
 	+[eqrw] Elphant <23.2134704589844>
 	+[eqrw] Elesant <27.7278366088867>
 	+[eqrw] Elephanta <30.2710800170898>
 	+[eqlemma] Elefannten <0>
 	+[eqlemma] Elefant <0>
 	+[eqlemma] Elefanten <0>
 	+[eqlemma] Elefantin <0>
 	+[eqlemma] Elefantine <0>
 	+[eqlemma] Elephandten <0>
 	+[eqlemma] Elephant <0>
 	+[eqlemma] Elephanten <0>
 	+[eqlemma] Elesant <0>
 	+[eqlemma] elefant <0>
 	+[eqlemma] elephanten <0>

Here, the "eqpho" attribute contains all surface forms recognized as phonetic variants of the query term,
"eqrw" contains those surface forms recognized as variants by the heuristic rewrite cascade,
and "eqlemma" contains the surface forms most likely to be mapped to the same modern lemma as the query term.
This online expansion strategy
is used by the L<DTA Query Lizard|http://kaskade.dwds.de/dstar/dta/lizard?q=Elephant>,
and was also used by an earlier the DTA corpus index
as described in L<Jurish et al. (2014)|http://ceur-ws.org/Vol-1131/mindthegap14_7.pdf>,
but has since been replaced there by an online lemmatization query using the "lemma" expander,
in conjunction with a direct query of the underlying corpus $Lemma index.

=cut

##========================================================================
## Analysis Chains: Format Conversion
=pod

=head3 Format Conversion

The CAB server can be used to convert between various
supported I/O L<Formats|/Formats>.  In this mode,
no analysis is performed on the input data
(with the exception of tokenization for raw untokenized input),
but the input document is parsed and re-formatted according to
the selected output format.  The analysis chain "null" can be
selected for such tasks.  To tokenize a simple text
string for instance, you can select the "null" analyzer and the
"text" format, and expect output such as
L<this|http://www.deutschestextarchiv.de/demo/cab/?a=null&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>.

This mode of operation is mostly useful in conjunction with
L<file upload queries|/A File Query> to convert analyzed files.
If you only need to tokenize raw text files, consider using
the more efficient L<WASTE tokenizer web-service|http://www.dwds.de/waste/>
directly,
or the L<WebLicht|http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/> tool-chainer,
which offers a number of different tokenizer components.

=cut


##========================================================================
## Formats
=pod

=head2 Formats

The CAB web-service supports a number of different I/O formats for
document data.  This section presents a brief outline of some of the more
popular formats.  See L<DTA::CAB::Format/SUBCLASSES> for a list of currently
implemented format subclasses, and see
the "L<Formats|http://www.deutschestextarchiv.de/demo/cab/formats>" link
in the CAB web-service interface link area for a list of format aliases supported
by the server.

=cut

##========================================================================
## Formats: Text-based
=pod

=head3 Text-based Formats

CAB supports various text-based formats for human consumption and/or further processing.
While typically not as flexible or efficient as the "pure" data-oriented formats
described L<below|/Data-oriented formats>, CAB's native text-based formats offer
a reasonable compromise between human- and machine-readability.
All text-based CAB formats expect and return data encoded in L<UTF-8|https://en.wikipedia.org/wiki/UTF-8>,
B<without> a L<Byte-order mark|https://en.wikipedia.org/wiki/Byte_order_mark>.

=cut


##========================================================================
## Formats: Text-based: Text
=pod

=head4 L<Text|DTA::CAB::Format::Text>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=text&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Simple human-readable text format as described under "L<Basic Usage|/Basic Usage>", above.
Blank lines indicate sentence boundaries,
comments are lines beginning with "%%",
a line with no leading whitespace contains the surface text of a new token (word),
and subsequent token-lines are attribute values beginning with a TAB character
and a plus sign ("+"), followed by the attribute label enclosed in square brackets
"[...]" and the attribute value as a text-string.

Primarily useful for direct inspection and debugging.

=cut

##========================================================================
## Formats: Text-based: CSV
=pod

=head4 L<CSV|DTA::CAB::Format::CSV>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=csv&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Simple fixed-width "vertical" text format containing only selected
attribute values.
Each line is either a
comment introduced by "%%", an empty line indicating a sentence boundary,
or a TAB-separated token line.  Token lines are of the form

 SURFACE_TEXT   XLIT_TEXT   CANON_TEXT    POS_TAG    LEMMA	?DETAILS

where I<SURFACE_TEXT> is the surface form of the token,
I<XLIT_TEXT> is the result of a simple deterministic transliteration
using L<unicruft|http://odo.dwds.de/~jurish/software/unicruft/>,
I<CANON_TEXT> is the automatically determined canonical modern form
for the token, I<POS_TAG> is the part-of-speech tag assigned
by the L<moot|http://kaskade.dwds.de/~jurish/projects/moot/> tagger,
I<LEMMA> is the modern lemma form determined for I<CANON_TEXT> and I<POS_TAG>
by the L<TAGH morphological analyzer|http://www.tagh.de/>,
and I<DETAILS> if present are additional details.

This is the most compact of the text-based formats supported by CAB,
but lacks flexibility.

=cut


##========================================================================
## Formats: Text-based: TT
=pod

=head4 L<TT|DTA::CAB::Format::TT>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=tt&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Simple machine-readable "vertical" text format similar to that used by
L<Corpus WorkBench|http://cwb.sourceforge.net/>.  Each line is either a
comment introduced by "%%", an empty line indicating a sentence boundary,
or a TAB-separated token line.  Token lines' initial column is the token
surface text, and subsequent columns are
are the token's attribute values, where each attribute value column
begins with the attribute label enclosed in square brackets
"[...]" and is followed by the attribute value as a text string.

Useful for further quick and dirty script-based processing.

=cut

##========================================================================
## Formats: Text-based: TJ
=pod

=head4 L<TJ|DTA::CAB::Format::TJ>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=tj&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Simple machine-readable "vertical" text format based on the L<TT|/TT>
format but using L<JSON|http://json.org/> to encode sentence- and token-level attributes
rather than an explicit attribute labelling scheme.
Each line is either a
comment introduced by "%%", an empty line indicating a sentence boundary,
a document attribute line,
a sentence attribute line,
or a TAB-separated token line.
Document-attribute lines are comments of the form
"C<%%$TJ:DOC=>I<JSON>", where I<JSON> is a JSON object representing
auxilliary document attributes.
Sentence-attribute lines are analogousd comments of the form
"C<%%$TJ:SENT=>I<JSON>".
Token lines consist of the the token surface text,
followed by a TAB character,
followed by a JSON object representing the internal token structure.

Useful for further script-based processing.

=cut


##========================================================================
## Formats: XML-based
=pod

=head3 XML-based Formats

The CAB web-service supports a number of XML-based formats for data exchange.
XML data formats are in general less efficient to parse and/or generate
than L<text-based|/Text-based Formats> or L<data-oriented|/Data-oriented Formats> formats,
but they do retain some degree of human-readability and the easy availability
of XML processing software packages such as L<libxml|http://www.xmlsoft.org/>
or L<XMLStarlet|http://xmlstar.sourceforge.net/>
makes such formats a reasonable candidate for archiving and cross-platform data sharing.

=cut

##========================================================================
## Formats: XML-based: XmlTokWrap
=pod

=head4 L<XmlTokWrap|DTA::CAB::Format::XmlTokWrap>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=twxml&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Simple serial XML-based format as used by the L<DTA::TokWrap|http://odo.dwds.de/~moocow/software/dta-tokwrap/>
module.  Supports arbitrary token attribute substructure, but fairly slow.

=cut

##========================================================================
## Formats: XML-based: XmlTokWrapFast
=pod

=head4 L<XmlTokWrapFast|DTA::CAB::Format::XmlTokWrapFast>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=ftwxml&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Simple serial XML-based format as used by the L<DTA::TokWrap|http://odo.dwds.de/~moocow/software/dta-tokwrap/>
module.  Faster than the L<XmlTokWrap|/XmlTokWrap> formatter, but doesn't support all attributes.

=cut

##========================================================================
## Formats: XML-based: TEI
=pod

=head4 L<TEI|DTA::CAB::Format::TEI>

[L<example input|http://./elephant.tei-xml>,
L<example output|http://www.deutschestextarchiv.de/demo/cab/query?fmt=tei&raw=1&qd=%3C%3Fxml+version%3D%221.0%22+encoding%3D%22UTF-8%22%3F%3E%0A%3CTEI%3E%0A+%3Ctext%3E%0A+++%3C%21--+running+headers+are+ignored+by+the+tokenizer+--%3E%0A+++%3Cfw%3ERunning+header%3C%2Ffw%3E%0A+++%3Cp%3EEJn+zamer+Elephant+gillt+ohngef%C3%A4hr+zweyhundert+Thaler%3C%2Fp%3E%0A+++%3C%21--+paragraph+boundaries+imply+sentence+boundaries%2C+even+without+punctuation+--%3E%0A+++%3Cp%3ECeterum+censeo+Carthaginem+esse+delendam%3C%2Fp%3E%0A+%3C%2Ftext%3E%0A%3C%2FTEI%3E%0A>]

Parses raw un-tokenized TEI-like XML input (with or without //c elements) using
L<DTA::TokWrap|http://odo.dwds.de/~moocow/software/dta-tokwrap/> to reserialize and tokenize the
source text, and splices analysis results into the resulting XML document
using the L<XmlTokWrap|/XmlTokWrap> format.
Any C<E<lt>sE<gt>> or C<E<lt>sE<gt>> elements in the input will be ignored and the input will be (re-)tokenized.
Output data is itself parseable by by the L<TEIws|/TEIws> formatter.

B<Be warned> that output sentence- and token-nodes (E<lt>sE<gt> and E<lt>wE<gt> elements, respectively)
may be I<fragmented> in the final output file.  A "fragmented" node
in this sense is a logical unit (sentence or token) realized in the output TEI-XML file as multiple
elements.  Fragmented nodes are encoded using the TEI "linking" attributes
L<@prev|http://www.tei-c.org/release/doc/tei-p5-doc/de/html/ref-att.global.linking.html#tei_att.prev>
and
L<@next|http://www.tei-c.org/release/doc/tei-p5-doc/de/html/ref-att.global.linking.html#tei_att.next>,
and only the first element of a fragmented node should contain the CAB attribute substructure for
that node.

Input to this class need not strictly conform to the L<TEI Guidelines|http://www.tei-c.org/>;
in fact, the only structural requirement is that at least one C<E<lt>textE<gt>> element be present --
any input outside of the scope of a C<E<lt>textE<gt>> element is ignored.  Input files must however
be encoded in L<UTF-8|https://en.wikipedia.org/wiki/UTF-8>.

Primarily useful for analyzing native TEI-like XML corpus data without losing
structural information encoded in the source XML itself.

=cut

##========================================================================
## Formats: XML-based: TEIws
=pod

=head4 L<TEIws|DTA::CAB::Format::TEIws>

[L<example input|http://./elephant.teiws-xml>,
L<example output|http://www.deutschestextarchiv.de/demo/cab/query?fmt=tei&raw=1&qd=%3C%3Fxml+version%3D%221.0%22+encoding%3D%22UTF-8%22%3F%3E%0A%3CTEI%3E%0A+%3Ctext%3E%0A+++%3C%21--+running+headers+are+ignored+by+the+tokenizer+--%3E%0A+++%3Cfw%3ERunning+header%3C%2Ffw%3E%0A+++%3Cp%3EEJn+zamer+Elephant+gillt+ohngef%C3%A4hr+zweyhundert+Thaler%3C%2Fp%3E%0A+++%3C%21--+paragraph+boundaries+imply+sentence+boundaries%2C+even+without+punctuation+--%3E%0A+++%3Cp%3ECeterum+censeo+Carthaginem+esse+delendam%3C%2Fp%3E%0A+%3C%2Ftext%3E%0A%3C%2FTEI%3E%0A>
L<example output 2|http://www.deutschestextarchiv.de/demo/cab/query?fmt=teiws&raw=1&qd=%3C%3Fxml+version%3D%221.0%22+encoding%3D%22UTF-8%22%3F%3E%0A%3CTEI%3E%0A++%3Ctext%3E%0A++++%3C%21--+running+headers+are+ignored+by+the+tokenizer+--%3E%0A++++%3Cfw%3ERunning+header%3C%2Ffw%3E%0A++++%3Cp%3E%0A++++++%3Cs%3E%0A++++++++%3Cw%3EEJn%3C%2Fw%3E%0A++++++++%3Cw%3Ezamer%3C%2Fw%3E%0A++++++++%3Cw%3EElephant%3C%2Fw%3E%0A++++++++%3Cw%3Egillt%3C%2Fw%3E%0A++++++++%3Cw%3Eohngef%C3%A4hr%3C%2Fw%3E%0A++++++++%3Cw%3Ezweyhundert%3C%2Fw%3E%0A++++++++%3Cw%3EThaler%3C%2Fw%3E%0A++++++%3C%2Fs%3E%0A++++%3C%2Fp%3E%0A++++%3C%21--+paragraph+boundaries+imply+sentence+boundaries%2C+even+without+punctuation+--%3E%0A++++%3Cp%3E%0A++++++%3Cs%3E%0A++++++++%3Cw%3ECeterum%3C%2Fw%3E%0A++++++++%3Cw%3Ecenseo%3C%2Fw%3E%0A++++++++%3Cw%3ECarthaginem%3C%2Fw%3E%0A++++++++%3Cw%3Eesse%3C%2Fw%3E%0A++++++++%3Cw%3Edelendam%3C%2Fw%3E%0A++++++%3C%2Fs%3E%0A++++%3C%2Fp%3E%0A++%3C%2Ftext%3E%0A%3C%2FTEI%3E%0A>
]

High-level parser/formatter class for pre-tokenized (and possibly fragmented) TEI-like XML
as output by the L<TEI|/TEI> formatter.
Input files should be encoded in L<UTF-8|https://en.wikipedia.org/wiki/UTF-8>.

Potentially useful for analyzing pre-tokenized TEI-like XML data,
but primarily used for converting to other, script-friendlier formats
such as L<CSV|/CSV>.

=cut

##========================================================================
## Formats: XML-based: TCF
=pod

=head4 L<TCF|DTA::CAB::Format::TCF>

[L<example input (untokenized)|http://./elephant.raw.tcf>,
L<example input (pre-tokenized)|http://./elephant.tok.tcf>,
L<example output|http://www.deutschestextarchiv.de/demo/cab/?fmt=tcf&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Monolithic stand-off XML format used by CLARIN-D, in particlar by the
L<WebLicht|http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/> tool-chainer.
See L<the TCF format documentation|http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/The_TCF_Format>
for details on the TCF format.
CAB currently handles only the C<text>, C<tokens>, C<sentences>, C<POStags>, C<lemmas>, and C<orthography>
TCF layers.

=cut

##========================================================================
## Formats: XML-based: XML-RPC
=pod

=head4 L<XML-RPC|DTA::CAB::Format::XmlRpc>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=xmlrpc&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Flexible but obscenely inefficient format used for data transfer
by the L<XML-RPC Protocol|https://en.wikipedia.org/wiki/XML-RPC>.
Avoid it if you can.

=cut

##========================================================================
## Formats: Data-oriented
=pod

=head3 Data-oriented Formats

The following formats provide direct dumps of the
underlying L<DTA::CAB::Document|DTA::CAB/Data Model> used internally by CAB itself.
They are efficient to parse and to produce, may not be suitable for direct
human consumption.

=cut

##========================================================================
## Formats: Data-oriented: JSON
=pod

=head4 L<JSON|DTA::CAB::Format::JSON>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=json&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Direct L<JSON|http://json.org/> dump of the underlying
L<DTA::CAB::Document|DTA::CAB/Data Model> structure
using the Perl L<JSON::XS|https://metacpan.org/release/JSON-XS> module.
Very fast and flexible, suitable for further automated processing.

=cut

##========================================================================
## Formats: Data-oriented: YAML
=pod

=head4 L<YAML|DTA::CAB::Format::YAML>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=yaml&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Direct dump of the underlying
L<DTA::CAB::Document|DTA::CAB/Data Model> structure
as L<YAML|https://en.wikipedia.org/wiki/YAML> markup.
Fast, flexible, and supports shared substructures, unlike the L<JSON|/JSON> formatter.

=cut

##========================================================================
## Formats: Data-oriented: Perl
=pod

=head4 L<Perl|DTA::CAB::Format::Perl>

[L<example|http://www.deutschestextarchiv.de/demo/cab/?fmt=perl&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Direct dump of the underlying
L<DTA::CAB::Document|DTA::CAB/Data Model> structure
using the Perl L<Data::Dumper|https://metacpan.org/release/Data-Dumper> module.
Mainly useful for further automated processing with Perl
while retaining some degree of human readability.

=cut

##========================================================================
## Formats: Data-oriented: Storable
=pod

=head4 L<Storable|DTA::CAB::Format::Storable>

[L<example|http://www.deutschestextarchiv.de/demo/cab/query?fmt=bin&q=EJn%20zamer%20Elephant%20gillt%20ohngef%C3%A4hr%20zweyhundert%20Thaler.%20Ceterum%20censeo%20Carthaginem%20esse%20delendam.>]

Direct binary dump of the underlying
L<DTA::CAB::Document|DTA::CAB/Data Model> structure
using the Perl L<Storable|https://metacpan.org/release/Storable> module.
This is currently the fastest I/O class for both in- and output,
mainly useful for further automated processing with Perl.

=cut


##========================================================================
## References
=pod

=head1 REFERENCES

Jurish, B., C. Thomas, & F. Wiegand. "Querying the Deutsches Textarchiv."
In U. Kruschwitz, F. Hopfgartner, & C. Gurrin (editors),
Proceedings of the Workshop I<L<MindTheGap 2014: Beyond Single-Shot Text Queries: Bridging the Gap(s) between Research Communities|http://ceur-ws.org/Vol-1131/>>
Berlin, Germany, 4th March, 2014, pages 25-30, 2014.
URL L<http://ceur-ws.org/Vol-1131/mindthegap14_7.pdf|http://ceur-ws.org/Vol-1131/mindthegap14_7.pdf>

Jurish, B. I<Finite-state Canonicalization Techniques for Historical German.>
PhD thesis, Universität Potsdam, 2012 (defended 2011).
URN urn:nbn:de:kobv:517-opus-55789,
URL L<http://opus.kobv.de/ubp/volltexte/2012/5578/|http://opus.kobv.de/ubp/volltexte/2012/5578/>

=cut

##======================================================================
## Footer
##======================================================================

=pod

=head1 AUTHOR

Bryan Jurish E<lt>L<jurish@bbaw.de|mailto:jurish@bbaw.de>E<gt>

=head1 COPYRIGHT AND LICENSE

Copyright (C) 2015 by Bryan Jurish

This package is free software; you can redistribute it and/or modify
it under the same terms as Perl itself, either Perl version 5.20.2 or,
at your option, any later version of Perl 5 you may have available.

=cut
